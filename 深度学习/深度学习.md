## 多层感知机

前向传播

输入 激活函数 隐含层 激活函数 输出

反向传播

输出 计算误差 传递隐藏层的误差，更新权重  传递输入层的误差，更新权重



与准确率相关的几个关键参数

激活函数

损失函数

梯度下降方法

学习率的大小

正则化



## 激活函数的原理类别实现

- 作用

  1.解决线性不可分，达到非线性

  2.特征充分组合

- 种类

  sigmoid  0～1 缺点：梯度消失  sigmoid导数可能会很小，导致消失  不适用于深层网络

  tanh   -1 ～ 1   

  relu  max(0,x)  计算简单，收敛快    缺点：容易挂掉

  leaky relu

  maxout  

  softmax



## 损失函数

0-1损失

平房损失

绝对值

对数

全局

交叉墒损失



## 梯度下降

SGD

momentumSGD  利用动量，加速收敛

AdaGrad  自动变更学习率



## 学习率设定

tf指数学习率

论文

## 正则化

避免过拟合

1.参数添加约束  l1, l2

2.Dropout 

3.数据增强

4.提前停止

4.集成化训练 bagging boosting

5.对抗训练

